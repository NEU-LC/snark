#!/usr/bin/env python3

# This file is part of snark, a generic and flexible library
# Copyright (c) 2011 The University of Sydney
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
# 3. Neither the name of the University of Sydney nor the
#    names of its contributors may be used to endorse or promote products
#    derived from this software without specific prior written permission.
#
# NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE
# GRANTED BY THIS LICENSE.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT
# HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
# BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
# IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

import argparse
import gzip
import logging
import os
import signal
import sys
# import warnings
from time import gmtime
from time import time as get_time

import numpy as np

try:
    import os.errno as errno
except ModuleNotFoundError:
    import errno

__author__ = 'v.vlaskine'

description = """

A thin trivial convenience wrapper for tensorflow.session.run().
Read tensors as binary on stdin, load model from file, apply model, write output tensors to stdout.
For more on session export/import: see https://www.tensorflow.org/programmers_guide/meta_graph .
To control inference device set CUDA_VISIBLE_DEVICES=<GPU number, or empty for CPU> .
"""

epilog = """
examples
    main use cases
        each record contains only tensor
            cat input-tensors.bin | tensor-cat --session model > output-tensors.bin

        each record in input data has header followed by tensor and then footer (e.g. containing timestamp, sequence number, or alike)
            cat header.input-tensor.footer.bin | tensor-cat --header-size 24 --footer-size 16 ... > header.footer.output-tensor.bin

        running on mnist
            # train and export mnist model
            tensorflow/examples/tensorflow-mnist-train-and-export

            # output predictions for first 10 images
            cat MNIST_data/t10k-images-idx3-ubyte.gz \\
                | gunzip \\
                | tail -c+17 \\
                | cv-cat --input "rows=28;cols=28;type=ub;no-header" "convert-to=f,0.0039" \\
                | tensor-cat --session session --dir mnist -i x -o argmax \\
                | csv-from-bin l \\
                | head

            # save first 10 images as png and view to compare with the max output among the predictions
            cat MNIST_data/t10k-images-idx3-ubyte.gz | gunzip | tail -c+17 \\
                | cv-cat --input "rows=28;cols=28;type=ub;no-header" "head=10" \\
                | cv-cat --input "rows=28;cols=28;type=ub;no-header" "file=png;null"
            eog *.png

            # visualise e.g. the first convolution activations
            cat MNIST_data/t10k-images-idx3-ubyte.gz \\
                | gunzip \\
                | tail -c+17 \\
                | cv-cat --input "rows=28;cols=28;type=ub;no-header" "convert-to=f,0.0039" \\
                | tensor-cat --session session --dir mnist -i x -o h_pool1 \\
                | math-array transpose --shape 14,14,32 --to 2,0,1 \\
                | cv-cat --fps 1 --input "no-header;rows=$((32*14));cols=14;type=f" "untile=8,4;resize=4;view;null"

    output to files
        the following command will output each channel of each layer to in subdirectories of current directory files (see below)
            cat input.bin | tensor-cat ... --output layer1,layer2 --output-to-files
        files would like like
            0/layer1/0.bin
            0/layer2/0.bin
            0/layer2/1.bin
            ...
            1/layer1/0.bin
            1/layer2/0.bin
            1/layer2/1.bin
            ...


"""


def parse_args(argv=None):
    parser = argparse.ArgumentParser(
        description=description, epilog=epilog, formatter_class=argparse.RawTextHelpFormatter)

    input_options = parser.add_argument_group('input options')
    input_options.add_argument(
        '--batch-size', type=int, default=1,
        help='todo: process input in batches of given size; default: %(default)i')
    input_options.add_argument(
        '--footer-size', type=int, default=0,
        help='if a footer of given size in bytes is present, pass it to stdout with output tensor prepended')
    input_options.add_argument(
        '--header-size', type=int, default=0,
        help='if a header of given size in bytes is present, pass it to stdout with output tensor appended')
    input_options.add_argument(
        '--input', '-i', type=str, help='input layer name; default: %(default)s')
    input_options.add_argument(
        '--input-shape', type=str, help='shape of a single input record as a comma-separated list')

    model_load_options = parser.add_argument_group('computational graph options')
    model_load_options.add_argument(
        '--frozen-graph', '--graph', type=str, help='path to frozen .pb graph file')
    model_load_options.add_argument(
        '--keep-devices', action="store_true",
        help='do not clear devices on graph load, i.e. you plan to run inference in the same environment as training; has effect only in tensorflow 1.0 and higher')
    model_load_options.add_argument(
        '--keras-model', type=str, help='path to .json model file; h5 support: todo, just ask')
    model_load_options.add_argument(
        '--keras-weights', type=str, help='path to .h5 weight file')
    model_load_options.add_argument(
        '--session', default='session', type=str,
        help='saved session filename (without extension); default: %(default)s')
    model_load_options.add_argument(
        '--session-dir', '--dir', default='.',
        type=str, help='saved session directory; default: %(default)s')
    # parser.add_argument( '--session-graph', '--graph', '--graph-file', default = 'session.meta', type = str, help = 'path to saved graph file; default: %(default)s' )
    model_load_options.add_argument(
        '--session-graph', type=str, help='path to saved graph file')
    model_load_options.add_argument(
        '--variable', '--var', nargs='*', type=str,
        help='variables to pass to the model as name-value pairs, e.g: "y:5", "dim:[3,4,5]", etc')

    gpu_options = parser.add_argument_group('gpu options')
    # parser.add_argument( '--device', type = str, help = 'device to use, e.g. /cpu:0, /device:GPU:0, etc' )# parser.add_argument( '--device', type = str, default = "/cpu:0", help = 'device to use, e.g. /cpu:0, /device:GPU:0, etc; default: %(default)s' )
    gpu_options.add_argument('--gpu-allow-growth', action="store_true",
                             help='ATTENTION: for keras model always is set to true')
    gpu_options.add_argument('--gpu-virtual-memory', type=float,
                             help='(tf2 only) GPU memory to allocate in GBs. Use about 80%% of memory on laptops to bypass some CUDNN out of memory bugs.')
    # --gpu-virtual-memory: for some reason, allocating less memory for the virtual GPU fixes CUDNN out of memory bugs when creating graph.

    output_options = parser.add_argument_group('output options')
    output_options.add_argument(
        '--keep-input', action="store_true", help='append output to input')
    output_options.add_argument(
        '--output', '-o', nargs='+', type=str,
        help='output layer name; multiple space- or comma-separated outputs accepted; default: %(default)s')
    output_options.add_argument(
        '--output-as-gzip', '--gzip', action="store_true",
        help='if --output-to-files, then gzip each channel to file ./<input-number>/<layer-name>/<channel-number>.bin.gz')
    output_options.add_argument(
        '--output-dirs', '--dirs', type=str,
        help='list of output directories, one for each input instead of ./<input-number>/... (see --output-to-files)')
    output_options.add_argument(
        '--output-to-files', action="store_true",
        help='convenience option; for each input, output each channel to file ./<input-number>/<layer-name>/<channel-number>.bin (see examples)')
    output_options.add_argument(
        '--output-channels-max', type=str,
        help='convenience option; if --output-to-files, comma-separated list: output not more than a given number of layer channels')
    output_options.add_argument(
        '--output-channels-transpose', '--transpose', action="store_true",
        help='convenience option; output 3D tensors transposed as 2,1,0')
    output_options.add_argument(
        '--png', action="store_true",
        help='convenience option; for each input, output each channel in human-viewable form to file ./<input-number>/<layer-name>/<channel-number>.png (see examples)')
    output_options.add_argument(
        '--png-min', type=float, help='if --png, min value to use for normalizing tensor elements')
    output_options.add_argument(
        '--png-max', type=float, help='if --png, min value to use for normalizing tensor elements')

    graph_query_options = parser.add_argument_group('graph query options')
    graph_query_options.add_argument(
        '--get-operations', '--get-operation-names', '--operation-names',
        '--operations', action="store_true", help='load graph, print operation names, and exit')
    graph_query_options.add_argument(
        '--tensors', '--tensor', type=str,
        help='comma-separated list of tensor names, if using tensor options; e.g. "image_tensor:0"')
    graph_query_options.add_argument(
        '--tensor-shape', action="store_true", help='print tensor shape and exit')
    graph_query_options.add_argument(
        '--tensor-type', '--tensor-dtype', action="store_true", help='print tensor type and exit')

    verbosity_options = parser.add_argument_group('verbosity options')
    verbosity_options.add_argument(
        '--min-log-level', '--log-level', type=str, default="2",
        help='min log level (as os.environ[\'TF_CPP_MIN_LOG_LEVEL\']); default: %(default)s')
    verbosity_options.add_argument(
        '--verbose', '-v',
        action='store_const', const=logging.INFO,
        dest='loglevel', help="verbose output to stderr")

    args = parser.parse_args(argv)

    # Argument checking
    if not (args.tensor_shape or args.tensor_type or args.get_operations) and not (args.input or args.output):
        die("Select operation (--input-shape, --input-tensor or --get-operations) "
            "or provide --input and --output to run inference.")
    if not args.frozen_graph and not args.keras_model:
        die("expected one of --keras-model or --frozen-graph")
    elif args.frozen_graph and args.keras_model:
        die("expected either --keras-model or --frozen-graph; got both")
    elif not args.get_operations:  # Inference mode
        if args.tensor_type or args.tensor_shape:
            if not args.tensors:
                die("To use --tensor-shape or --tensor-type please specify --tensor.")
        elif not args.input or not args.output:
            die("provide --input and --output to run inference.")
    return args


def die(m):
    logging.critical("tensor-cat:" + str(m))
    exit(1)


def handle_signal(s, f):
    logging.warning("tensor-cat: broken pipe, exit")
    exit(0)

# todo: quick and dirty; failed to find how to read from stdin using tf.FixedLengthRecordReader
def iterator(file, input_tensor, header_size, footer_size):
    shape = (input_tensor.get_shape() if TF_VERSION[0] < 1 else input_tensor.shape).as_list()
    if ARGS.input_shape is None:
        shape = [1 if v is None else v for v in shape]  # quick and dirty
        logging.warning(f"input shape: --input-shape not defined, tried to deduce it as: {shape}")
    else:
        input_shape = eval("[" + ARGS.input_shape + "]")
        if len(shape) != len(input_shape):
            die("expected input shape dimensions " + str(len(shape)) + ", got "
                + str(len(input_shape)) + " in --input-shape: [" + ARGS.input_shape + "]")
        for i in range(len(shape)):
            if shape[i] is None:
                shape[i] = input_shape[i]
            elif shape[i] != input_shape[i]:
                die("expected input shape dimension " + str(i) + ": " + str(shape[i]) + ", got " + str(
                    input_shape[i]) + " in --input-shape: [" + ARGS.input_shape + "]")
    logging.info(f"using input shape: {shape}")
    ntype = (input_tensor._dtype if TF_VERSION[0] < 1 else input_tensor.dtype).as_numpy_dtype()
    size = np.prod(shape)
    itemsize = np.array([0], dtype=ntype).itemsize  # quick and dirty
    data_bytes = size * itemsize
    header = np.zeros((header_size), dtype=np.uint8)
    # buf = np.zeros((size), dtype=ntype)
    data = np.zeros(shape, dtype=ntype)
    footer = np.zeros((footer_size), dtype=np.uint8)
    while True:
        if file.readinto(header) != header_size or file.readinto(data) != data_bytes or file.readinto(footer) != footer_size:
            yield None, None, None
        yield header, data, footer

def variables(v):
    if v is None:
        return {}
    d = {}
    for i in v:
        s = i.split(':')
        d[tf.compat.v1.get_collection(s[0].strip())[0]] = eval(s[1])
    return d

def parse_output_names(output_names):
    if output_names and len(output_names) == 1:
        return output_names[0].split(',')
    return output_names or []

def shape_output(value):
    if value.shape[0] != 1: value = value[np.newaxis, ...]
    if value.ndim != 4: value = value[..., np.newaxis]
    return value

class tf1:
    class model:
        def __init__(self):
            self.config = tf.compat.v1.ConfigProto()
            self.config.gpu_options.allow_growth = ARGS.gpu_allow_growth
            self.session = None
            self.session_meta = None

        def load_model(self, frozen_graph=None, keras_model=None, keras_weights=None):
            if frozen_graph is not None:
                logging.info("loading frozen graph from '" + frozen_graph + "'...")
                with tf.Graph().as_default() as graph:
                    graph_def = tf.GraphDef()
                    with tf.gfile.GFile(frozen_graph, 'rb') as fid:
                        serialized_graph = fid.read()
                        graph_def.ParseFromString(serialized_graph)
                        tf.import_graph_def(graph_def, name='')
                        logging.info("loaded frozen graph from '" + frozen_graph + "'")
                    self.session = tf.Session(graph=graph, config=self.config)
            elif keras_model is not None:
                logging.info("loading model from '" + keras_model + "'...")
                # # quick and dirty to make it backward compatible for now
                # self.config.gpu_options.allow_growth = True
                # quick and dirty, just to make it working
                self.session = tf.Session(config=self.config)
                tf.compat.v1.keras.backend.set_session(self.session)
                if keras_model.split('.')[-1] == 'h5':
                    model = tf.keras.models.load_model(keras_model, custom_objects={'tf': tf}, compile=False)
                    model.load_weights(keras_model)
                else:
                    with open(keras_model, 'r') as f:
                        model = tf.keras.models.model_from_json(f.read(), custom_objects={'tf': tf})
                    if not keras_weights and not (ARGS.get_operations or ARGS.tensor_shape or ARGS.tensor_type):
                        logging.warning("Keras model definition loaded, but weights not specified")
                    else:
                        logging.info("loading weights from '" + keras_weights + "'...")
                        model.load_weights(keras_weights)
                self.session = tf.compat.v1.keras.backend.get_session()
            else:  # todo! there is an intermittent problem that prevents this branch from working; looking into it, but any help is welcome...
                logging.warning("there is an intermittent problem that prevents loading from checkpoint from working.")
                logging.warning("if loading with --session does not work, use tensor-calc graph-freeze to freeze the graph and run tensor-cat --frozen-graph ...; loading from checkpoint: todo")
                tf.reset_default_graph()  # voodoo?
                session_file = ARGS.session_dir + '/' + ARGS.session  # TODO: Is there a need for session_dir?
                # TODO: Clean up ARGS, pass to function?
                self.session_meta = ARGS.session_graph if ARGS.session_graph is not None else session_file + '.meta'
                if TF_VERSION[0] < 1 and ARGS.keep_devices:
                    logging.warning("--keep-devices specified, but will have no effect: not implemented in tensorflow version "
                         + tf.__version__ + " that you are running")
                logging.info("importing meta graph file from '" + self.session_meta + "'...")
                saver = tf.train.import_meta_graph(self.session_meta) if TF_VERSION[0] < 1 \
                    else tf.train.import_meta_graph(self.session_meta, clear_devices=not ARGS.keep_devices)
                logging.info("imported meta graph file from '" + self.session_meta + "'")
                self.session = tf.get_default_session()  # Todo: check this works
                logging.info("restoring session from '" + session_file + "'...")
                saver.restore(self.session, session_file)
                logging.info("restored session from '" + session_file + "'")
            # with tf.Session if args.frozen_graph is None else tf.Session( graph = graph ) as session: #with tf.Session( config = tf.compat.v1.ConfigProto( log_device_placement = True ) ) if args.frozen_graph is None else tf.Session( graph = graph, config = tf.compat.v1.ConfigProto( log_device_placement = True ) ) as session:
            # with tf.Session( config = tf.compat.v1.ConfigProto( log_device_placement = True ) ) if args.frozen_graph is None else tf.Session( graph = graph, config = tf.compat.v1.ConfigProto( log_device_placement = True ) ) as session:
            if ARGS.get_operations:
                return
            output_names = ARGS.output
            if ARGS.output and len(ARGS.output) == 1:
                output_names = ARGS.output[0].split(',')
            if ARGS.get_operations or ARGS.tensor_shape or ARGS.tensor_type:
                return
            self._outputs = [self.get_tensor(name) for name in output_names]
            self._input = self.get_tensor(ARGS.input)
            self.keep_prob = self.get_tensor('keep_prob', True)  # todo! quick and dirty
            self.feeds = variables(ARGS.variable)

        def get_tensor(self, name, permissive=False):
            try:
                return self.session.graph.get_tensor_by_name(name)
            except (KeyError, ValueError):
                logging.info("no tensor '" + name + "' in graph, checking collection...")
                entries = tf.compat.v1.get_collection(name)
                if len(entries) == 0:
                    if permissive:
                        logging.warning("graph does not have entry named '" + name + "'; skipped")
                        return None
                    else:
                        die("graph does not have entry named '" + name + "'")
                if len(entries) > 1:
                    die("expected entry size 1 for entry named '"
                        + name + "'; got: " + str(len(entries)) + "(todo?)")
                logging.info("tensor '" + name + "' found in collection")
                return entries[0]

        def get_operations(self):
            return self.session.graph.get_operations()

        def run(self, data):
            feeds = {self._input: data}
            if self.keep_prob:
                feeds[self.keep_prob] = 1.0
            return [shape_output(value) for value in self.session.run(self._outputs, feed_dict=feeds)]

        def get_input(self):
            return self._input

        def get_outputs(self):
            return self._outputs


class tf2:
    class model:
        def __init__(self):
            self._model = None
            self._input = None
            self._outputs = None
            self._tensors = None
            self._graph = None
            if len(tf.config.experimental.list_physical_devices('GPU')) == 1 and ARGS.gpu_virtual_memory:
                tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)
                tf.config.experimental.set_virtual_device_configuration(
                    tf.config.experimental.list_physical_devices('GPU')[0],
                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=int(1024 * ARGS.virtual_memory))])

        def load_model(self, frozen_graph=None, keras_model=None, keras_weights=None):
            if frozen_graph:
                graph_def = tf.compat.v1.GraphDef()
                graph_def.ParseFromString(open(frozen_graph, 'rb').read())
                def _imports_graph_def(): tf.compat.v1.import_graph_def(graph_def, name="")
                wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])
                self._graph = wrapped_import.graph
                if ARGS.get_operations or ARGS.tensor_shape or ARGS.tensor_type:
                    input_name = self._graph.get_operations()[0].name + ':0'
                    output_names = [self._graph.get_operations()[-1].name + ':0']
                else:
                    input_name = ARGS.input
                    output_names = parse_output_names(ARGS.output)
                self._model = wrapped_import.prune(
                    tf.nest.map_structure(self._graph.as_graph_element, input_name),
                    tf.nest.map_structure(self._graph.as_graph_element, output_names))
                self._input = self._model.inputs[0]
                self._outputs = self._model.outputs
            if keras_model:
                if keras_model.endswith(".h5"):
                    self._model = tf.keras.models.load_model(keras_model, custom_objects={'tf': tf}, compile=False)
                else:
                    with open(keras_model, 'r') as file:
                        self._model = tf.keras.models.model_from_json(file.read(), custom_objects={'tf': tf})
                    if not keras_weights and not (ARGS.get_operations or ARGS.tensor_shape or ARGS.tensor_type):
                        logging.warning("Keras model definition loaded, but weights not specified")
                if keras_weights: self._model.load_weights(keras_weights)
                # To work with --input and --outputs re-create the model with the desired tensors
                self._populate_tensors()
                if ARGS.get_operations or ARGS.tensor_shape or ARGS.tensor_type: return
                self._model = tf.keras.models.Model(self.get_input(), self.get_outputs())
                self._populate_tensors()

        def _populate_tensors(self):
            tensors = []
            for layer in self._model.layers:
                if isinstance(layer.input, list): tensors += layer.input
                else: tensors.append(layer.input)
                if isinstance(layer.output, list): tensors += layer.output
                else: tensors.append(layer.output)
            self._tensors = dict(zip([tensor.name for tensor in tensors], tensors))
            try:
                self._input = self.get_tensor(ARGS.input, permissive=True)
                output_names = parse_output_names(ARGS.output)
                self._outputs = [self.get_tensor(name, permissive=True) for name in output_names]
            except (TypeError, KeyError):
                if not ARGS.get_operations: raise

        def get_tensor(self, name, permissive=False):
            try:
                if self._graph:
                    return self._graph.get_tensor_by_name(name)
                return self._tensors[name]
            except KeyError:
                if not permissive: raise

        def get_outputs(self):
            return self._outputs

        def get_input(self):
            return self._input

        def get_operations(self):
            if ARGS.frozen_graph: return self._graph.get_operations()
            operations = list()
            unique_ops = set()
            for operation in self._tensors.values():
                if operation not in unique_ops:
                    unique_ops.add(operation)
                    operations.append(operation)
            return operations

        def run(self, data):
            return [shape_output(value.numpy()) for value in self._model(tf.convert_to_tensor(data))]


def main(args):
    global tf, TF_VERSION
    signal.signal(signal.SIGPIPE, handle_signal)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = args.min_log_level
    # with warnings.catch_warnings():  # Silence numpy deprecation warnings
        # warnings.simplefilter('ignore')
        # import tensorflow as tf
    import tensorflow as tf
    logging.info("imported tensorflow version: " + tf.__version__)
    TF_VERSION = [int(field) for field in tf.__version__.split('.') if str.isdigit(field)]
    if TF_VERSION[0] == 1:  # TODO: Is this needed since moving to python logging?
        tf.compat.v1.logging.set_verbosity(tf.logging.INFO if args.loglevel else tf.compat.v1.logging.WARN)
    # with tf.device( args.device ) as device: run() # todo: tf.device() does not quite work
    # todo: load from checkpoint

    model = tf2.model() if TF_VERSION[0] >= 2 else tf1.model()
    model.load_model(
        frozen_graph=args.frozen_graph, keras_model=args.keras_model, keras_weights=args.keras_weights)

    # Tensor and operations information
    if ARGS.get_operations:  # TODO: Examine, determine better segmentation of functions
        for o in model.get_operations():
            print(o.name)
        exit(0)
    if args.tensor_shape or args.tensor_type:
        if args.tensors:
            if args.tensor_shape and args.tensor_type:
                for tensor in args.tensors.split(','):
                    print(tensor + ',' + model.get_tensor(tensor).dtype.name + ','
                          + ','.join(list(map(str, list(model.get_tensor(tensor).shape)))))
            elif args.tensor_shape:
                for tensor in args.tensors.split(','):
                    print(tensor + ','
                          + ','.join(list(map(str, list(model.get_tensor(tensor).shape)))))
            elif args.tensor_type:
                for tensor in args.tensors.split(','):
                    print(tensor + ',' + model.get_tensor(tensor).dtype.name)
            exit(0)
        else:
            die("To use --tensor-shape or --tensor-type please specify --tensor.")

    # Run model inference

    output_tensors = model.get_outputs()
    input_tensor = model.get_input()
    output_channels_max = [None] * len(output_tensors)
    if args.output_channels_max:
        s = args.output_channels_max.split(',')
        for i in range(len(s)):
            if s[i] != "":
                output_channels_max[i] = int(s[i])

    logging.info("input tensor:")
    logging.info(input_tensor)
    logging.info("output tensors:")
    logging.info(output_tensors)

    logging.info("running inference...")
    input_number = 0
    if (args.output_to_files or args.png) and args.output_dirs:
        def output_dirs_iterator(filename):
            f = open(filename, 'r')
            for line in f:
                line = line.strip()
                if len(line) > 0:
                    yield line
        output_dirs = output_dirs_iterator(args.output_dirs)

    with os.fdopen(sys.stdout.fileno(), "wb") as stdout, os.fdopen(sys.stdin.fileno(), "rb") as stdin:
        for header, data, footer in iterator(stdin, input_tensor, args.header_size, args.footer_size):
            if data is None: break
            values = model.run(data)
            if args.output_to_files or args.png:
                try:
                    output_dir = str(input_number) if args.output_dirs is None else next(output_dirs)
                except StopIteration:
                    die("got input record " + str(input_number + 1) + " but only "
                        + str(input_number) + " entries in '" + args.output_dirs + "'")
                try:
                    os.makedirs(output_dir)
                except OSError as ex:
                    if ex.errno != errno.EEXIST: raise
                for i, (output_tensor, value) in enumerate(zip(output_tensors, values)):
                    logging.info(f"output_tensor {output_tensor}")
                    logging.info(f"values[{i}]: shape ({value.shape}), shape ({value.dtype})")
                    try:
                        os.makedirs(output_dir + '/' + output_tensors[i].name)
                    except OSError as ex:
                        if ex.errno != errno.EEXIST: raise
                    if value.shape[0] != 1:
                        die("output to files: expected output tensor with shape (1,...); got tensor "
                            + output_tensor.name + " of shape: " + str(value.shape) + "; todo?")
                    if len(value.shape) > 4:
                        die("output to files: expected output tensor with no more than 4 dimensions; got tensor "
                            + output_tensor.name + " of shape: " + str(value.shape) + "; todo?")
                    v = value if len(value.shape) <= 3 else np.transpose(
                        value[0], axes=(2, 0, 1))  # quick and dirty
                    output_channels = v.shape[0] if output_channels_max[i] is None else output_channels_max[i]
                    
                    if args.output_to_files:
                        for j in range(output_channels):
                            n = output_dir + '/' + output_tensor.name + '/' + str(j) + '.bin'
                            with gzip.open(n + '.gz', 'wb') if args.output_as_gzip else open(n, 'wb') as f:
                                f.write(v[j].tobytes())
                                f.close()
                            # with open( n if args.output_as_gzip else n + '.gz', 'wb' ) as f: f.write( gzip.compress( v[j].tobytes() ) if args.output_as_gzip else v[j].tobytes() )
                    if args.png:
                        for j in range(output_channels):
                            n = np.min(v[j]) if args.png_min is None else args.png_min
                            x = np.max(v[j]) if args.png_max is None else args.png_max
                            image = np.zeros(v[j].shape, dtype=np.uint8) if x == n else np.array(
                                (((np.array(v[j], dtype=np.float32) - n) / (x - n)) * 255), dtype=np.uint8)
                            from PIL import Image
                            Image.fromarray(image).save(output_dir + '/' + output_tensor.name + '/' + str(j) + '.png')
                input_number += 1
            else:
                stdout.write(header.tobytes())
                if args.keep_input: stdout.write(data.tobytes())
                stdout.write(footer.tobytes())
                if args.output_channels_transpose:
                    for i, (output_tensor, value) in enumerate(zip(output_tensors, values)):
                        if value.shape[0] != 1:
                            die("transpose channels: expected output tensor with shape (1,...); got tensor "
                                + output_tensor.name + " of shape: " + str(value.shape) + "; todo?")
                        v = np.transpose(value[0], axes=(2, 0, 1)) if len( value.shape ) == 4 else values[i]  # quick and dirty
                        stdout.write(v.tobytes())
                else:
                    for value in values: stdout.write(value.tobytes())
                stdout.flush()


if __name__ == '__main__':
    ARGS = parse_args()
    logging.basicConfig(
        format='%(filename)s: %(asctime)s.%(msecs)d: %(levelname)s: %(message)s',
        datefmt='%Y%m%dT%H%M%S',
        level=ARGS.loglevel)
    logging.Formatter.converter = gmtime
    logging.info('Arguments:')
    logging.info(ARGS)
    main(ARGS)

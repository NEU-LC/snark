#!/usr/bin/env python3

# This file is part of snark, a generic and flexible library
# Copyright (c) 2011 The University of Sydney
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
# 3. Neither the name of the University of Sydney nor the
#    names of its contributors may be used to endorse or promote products
#    derived from this software without specific prior written permission.
#
# NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE
# GRANTED BY THIS LICENSE.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT
# HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
# BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
# IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

__author__ = 'alex wendel, james underwood, vsevolod vlaskine'

import argparse
import numpy as np
import signal
import sys
import os

from ctypes import *
import math
import random
from os.path import expanduser
from collections import namedtuple
import comma
import cv2
from time import time

description = """

read tensors as binary on stdin, load model from file, apply model, write output tensors to stdout

PREREQUISITES:
This code requires the darknet fork at https://github.com/AlexeyAB/darknet/ to be built and installed.
Since the code lacks an install script, the best way to install is:

    cd $DARKNET_SRC_DIR
    cp darknet /usr/local/bin/
    cp darknet.so /usr/local/lib/
    cp darknet.py $(python3 -c 'import site; print(site.getsitepackages()[0])')

"""

os.environ['DARKNET_PATH'] = '/usr/local/lib/'
from darknet import (
    do_nms_obj,
    free_detections,
    get_network_boxes,
    load_net_custom,
    predict_image,
    IMAGE,
)


def c_array(ctype, values):
    arr = (ctype*len(values))()
    assert sizeof(arr) == values.nbytes
    memmove(byref(arr), values.ctypes.data, values.nbytes)
    return arr

def array_to_image(arr, input_type):
    arr = arr if arr.shape[2] == 1 else arr[:, :, [2, 1, 0]]

    arr = arr.transpose(2,0,1).astype(np.float32)

    c = arr.shape[0]
    h = arr.shape[1]
    w = arr.shape[2]
    arr = arr.flatten()

    data = c_array(c_float, arr)

    im = IMAGE(w,h,c,data)

    return im

def detect(net, meta, image, input_type, thresh=.25, hier_thresh=.5, nms=.45):
    im = array_to_image(image, input_type)

    num = c_int(0)

    pnum = pointer(num)
    predict_image(net, im)

    letterbox = False
    dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, None, 0, pnum, letterbox)
    num = pnum[0]
    if (nms): do_nms_obj(dets, num, meta.classes, nms);

    res = []
    for j in range(num):
        for i in range(meta.classes):
            if dets[j].prob[i] > 0:
                b = dets[j].bbox
                res.append((meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))
    res = sorted(res, key=lambda x: -x[1])
    free_detections(dets, num)
    return res

def report_vram():
    import subprocess
    import xml.etree.ElementTree as ET

    # get info from nvidia-smi as an XML document
    outcome = subprocess.check_output(['nvidia-smi', '-q', '-x'])
    my_pid = os.getpid()
    smi_tree = ET.fromstring(outcome)

    # find process_info tag whose pid child tag has text equal to the pid of this process
    process_info = smi_tree.find(f'.//process_info/pid[.="{my_pid}"]..')

    vram = process_info.find('used_memory').text
    vram = vram.split()[0] # remove 'MiB'
    print(vram)

def say( m ): sys.stderr.write("darknet-cat: {}\n".format(m))

def warn( m ): sys.stderr.write("darknet-cat: warning: {}\n".format(m))

def die( m ): sys.stderr.write("darknet-cat: {}\n".format(m)); sys.exit( 1 )

def read_buffer(buffer, dtype, count):
    return np.frombuffer(buffer.read(count * np.dtype(dtype).itemsize), dtype)

def write_buffer(buffer, data):
    buffer.write(data.tobytes())

def iterator( buffer, shape, n_type, header_size, footer_size ): # todo: quick and dirty; failed to find how to read from stdin using tf.FixedLengthRecordReader
    size = np.prod( shape )
    while True:
        header = read_buffer( buffer, np.uint8, header_size)
        data = read_buffer( buffer, ntype, size )
        footer = read_buffer( buffer, np.uint8, footer_size )
        if data.shape[0] == size: yield header, data.reshape( shape ), footer
        else: yield None, None, None

if __name__ == '__main__':
    # The darknet library prints all sorts of stuff to stdout, which interferes
    # with piping the output of darknet-cat into another tool. To workaround
    # this, we redirect everything from darknet's stdout to stderr.
    # To make sure stuff we print in python goes to the real stdout, we
    # duplicate it before redirecting.
    stdout_copy = os.fdopen(os.dup(sys.stdout.fileno()), sys.stdout.mode)
    os.dup2(sys.stderr.fileno(), sys.stdout.fileno())
    sys.stdout = stdout_copy

    def handle_signal( s, f ): sys.stderr.write("darknet-cat: broken pipe, exit\n"); sys.exit( 0 )
    signal.signal( signal.SIGPIPE, handle_signal )
    parser = argparse.ArgumentParser( description = description, formatter_class = argparse.RawTextHelpFormatter )
    parser.add_argument( '--input-shape', type = str, required=True, help = 'image size in the the format width,height,channels' )
    parser.add_argument( '--footer-size', type = int, default = 0, help = 'if a footer of given size in bytes is present, pass it to stdout with output tensor prepended' )
    parser.add_argument( '--header-size', type = int, default = 0, help = 'if a header of given size in bytes is present, pass it to stdout with output tensor appended' )
    parser.add_argument( '--keep-input', action="store_true", help = 'append output to input' )
    parser.add_argument( '--threshold', type=float, default=0.25, help='detector threshold' )
    parser.add_argument( '--verbose', '-v', action="store_true", help = 'more output' )
    parser.add_argument( '--save-images', action="store_true", help = 'save to labelled images' )
    parser.add_argument( '--config', help="configuration (cfg) file required by darknet", required=True)
    parser.add_argument( '--weights', help="trained darknet weights file", required=True)
    parser.add_argument( '--classes', help="number of classes", required=True, type=int)
    parser.add_argument( '--input-file', help="file to read image data from", type=argparse.FileType('rb'), default=sys.stdin.buffer)
    parser.add_argument( '--output-file', help="file to write detection results to", type=argparse.FileType('wb'), default=sys.stdout.buffer)
    parser.add_argument( '--report-vram', help="report the GPU memory used to run an intstance of darknet-cat (as an integer in MiB), then exit.", action="store_true")
    args = parser.parse_args()

    # Calling load_net_custom with last param (batch) equal to 1, causes the network to
    # startup in inference mode and not allocate memory for training.
    net = load_net_custom(args.config.encode('utf-8'), args.weights.encode('utf-8'), 0, 1)

    if args.report_vram:
        report_vram()
        sys.exit(0)

    meta = namedtuple("meta", "classes names") # create meta class directly rather than requiring a file
    meta.classes=args.classes
    meta.names = ['{}'.format(x) for x in range(meta.classes)] # create class strings by enumerating integers, decodes back to integers below

    input_shape = np.array(args.input_shape.split(',')).astype(int)[[1,0,2]]

    ntype=np.float32

    img_counter = 0
    t0 = time()
    for header, data, footer in iterator( args.input_file, input_shape, ntype, args.header_size, args.footer_size ):
        if args.verbose:
            say("time per image: {}".format(time()-t0))
            t0=time()

        if data is None:
            break

        results = detect(net, meta, data, ntype, thresh=args.threshold)

        p1_list, p2_list = list(), list()

        for (label, likelihood, output) in results:
            write_buffer(args.output_file, header)
            if args.keep_input: write_buffer(args.output_file, data)
            write_buffer(args.output_file, footer)
            write_buffer(args.output_file, np.asarray( meta.names.index(label), np.uint32 )) # convert back from number as string
            write_buffer(args.output_file, np.asarray( likelihood, np.float32 ))
            write_buffer(args.output_file, np.asarray( output, np.float32 )) # todo: is this generic enough - is it always a label, likelihood, tuple-of-floats?
            args.output_file.flush()

            if args.save_images:
                x, y, w, h = np.array(output).astype(int)
                p1_list.append((x-w//2, y-h//2))
                p2_list.append((x+w//2, y+h//2))

        if args.save_images:
            data = data.copy()
            img = (data / np.max(data) * 255).astype(np.uint8)
            img = img if img.shape[2] == 3 else np.repeat(img, 3, axis=2)
            for p1, p2 in zip(p1_list, p2_list):
                cv2.rectangle(img, p1, p2, (0, 0, 255), 2)
            cv2.imwrite("%05d.png" % img_counter, img)
            img_counter += 1

        if args.verbose:
            say("processing time per image: {}".format(time()-t0))
